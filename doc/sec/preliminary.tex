

\section{Preliminary}

\subsection{Koopman Operator}

Consider a measure space $(\mathcal{M}, \mathcal{F}, \rho)$,
on which we define a dynamical system
\begin{equation*}
  x(n+1) = f(x(n)), \quad x(n) \in \mathcal{M}, \quad n \geq 0.
\end{equation*}

\begin{definition}
  Consider the Hilbert space defined by:
  \begin{equation*}
  L^2(\mathcal{M}, \rho) = \left\{ \psi: \mathcal{M} \rightarrow \mathbb{C}:
    \|\psi\|_{L^2(\mathcal{M},\rho)} < \infty \right\}.
  \end{equation*}
  An element $\psi \in L^2(\mathcal{M}, \rho)$ is called an \emph{observable}.
\end{definition}

\begin{definition}
  The \emph{Koopman operator} maps an observable $\psi \in L^2(\mathcal{M}, \rho)$
  to another observable $\mathcal{K} \psi$ defined by:
  \begin{equation*}
    \mathcal{K} \psi = \psi \circ f.
  \end{equation*}
\end{definition}

\begin{definition}
  Given an observable $\psi \in L^2(\mathcal{M}, \rho)$
  and $\mu \in \mathbb{C}$, if
  \begin{equation*}
    \mathcal{K} \psi = \mu \psi,
  \end{equation*}
  then $\psi$ is called the \emph{eigenfunction} of $\mathcal{K}$ with eigenvalue $\mu$.
\end{definition}

\begin{proposition}
  Suppose $\{\psi_k\}_k$ are the eigenfunctions of
  the Koopman operator $\mathcal{K}$
  with eigenvalues $\{\mu_k\}_k$.
  Given any observable $\psi \in L^2(\mathcal{M}, \rho)$,
  we have
  \begin{equation}
    \label{eq:koopman-mode-decomposition-one-step}
    \psi(x(n)) = \sum\limits_k \mu_k \alpha_k \psi_k(x(n-1)).
  \end{equation}
\end{proposition}

\begin{proof}
  By the definition of dynamical system and
  the Koopman operator,
  we have
  \begin{equation*}
    \psi(x(n)) = \psi(f(x(n-1))) = \mathcal{K} \psi(x(n-1)).
  \end{equation*}
  Since $\psi$ can be decomposed into a linear combination of
  eigenfunctions of $\mathcal{K}$, it follows that
  \begin{equation*}
    \psi(x(n)) = \mathcal{K} \sum\limits_k \alpha_k \psi_k(x(n-1))
    = \sum\limits_k \alpha_k \mu_k \psi_k(x(n-1)).  \qedhere
  \end{equation*}
\end{proof}

\begin{corollary}
  Suppose $\{\psi_k\}_k$ are the eigenfunctions of
  the Koopman operator $\mathcal{K}$
  with eigenvalues $\{\mu_k\}_k$.
  Given any observable $\psi \in L^2(\mathcal{M}, \rho)$,
  we have
  \begin{equation}
    \label{eq:koopman-mode-decomposition}
    \psi(x(n)) = \sum\limits_k \mu_k^n \alpha_k \psi_k(x(0)).
  \end{equation}
\end{corollary}

\begin{proof}
  For any $\psi_k(x(n))$, we have
  \begin{equation*}
    \psi_k(x(n)) = \psi_k(f(x(n-1)))
    = \mathcal{K}\psi_k(x(n-1))
    = \mu_k\psi_k(x(n-1)),
  \end{equation*}
  repeatedly applying this relation, we get
  \begin{equation*}
    \psi_k(x(n)) = \mu_k^n \psi_k(x(0)).
  \end{equation*}
  Substituting this into the expression for $\psi(x(n))$
  from the proposition yields (\ref{eq:koopman-mode-decomposition}).
\end{proof}

\subsection{The EDMD Algorithm}

The EDMD algorithm aims to find a finite-dimensional
representation of the Koopman operator $\mathcal{K}$.
We start by selecting a dictionary $D = \{\psi_1,\psi_2,\cdots,\psi_M\}$,
where
\begin{equation*}
  \psi_i: \mathcal{M} \rightarrow \mathbb{R},
  \quad \text{for} \quad i = 1,2,\cdots, M.
\end{equation*}
We consider the span $U(D) = \text{span}\{\psi_1,\cdots,\psi_M\} = \{a^T\Psi:a \in \mathbb{C}^M\}$,
where $\Psi = [\psi_1,\psi_2,\cdots,\psi_M]^T$.
Assuming that for all $\psi \in U(D)$,
$\mathcal{K} \psi \in U(D)$,
the Koopman operator $\mathcal{K}$
acts as a linear transformation in the space $U(D)$.
Thus, $\mathcal{K}$ can be represented
by a matrix $K \in \mathbb{R}^{M \times M}$:
\begin{equation*}
  \mathcal{K} \left[
    \begin{array}{cccc}
      \psi_1&\psi_2&\cdots&\psi_M
    \end{array}
  \right] = \left[
    \begin{array}{cccc}
      \psi_1&\psi_2&\cdots&\psi_M
    \end{array}
  \right]K,
\end{equation*}
and given $\psi \in U(D)$ with coordinates $a$,
the coordinates of $\mathcal{K} \psi$ are $Ka$.

However, due to the nonlinear properties of $f$,
the Koopman operator $\mathcal{K}$ is not always
exactly represented by the matrix $K$.
Nonetheless, $K$ can still serve as a finite-dimensional
approximation of $\mathcal{K}$.

\subsubsection{Algorithm}

From a data science perspective,
given a set of data points $\{(x(n), y(n))\}_{n = 1}^N$
with $y(n) = f(x(n))$,
we can find $K$ by solving the optimization problem:
\begin{equation}
  \label{eq:EDMD-problem}
  K = \text{argmin}_{\tilde{K} \in \mathbb{R}^{M \times M}} J(\tilde{K})
  = \sum\limits_{n = 1}^N \|\Psi(y(n)) - \tilde{K}^T \Psi(x(n))\|_2^2,
\end{equation}
where $\Psi(x) = [\psi_1(x), \psi_2(x),\cdots,\psi_M(x)]^T$.

\begin{proposition}
  The solution to (\ref{eq:EDMD-problem}) is
  \begin{equation*}
    K = (AG^+)^T,
  \end{equation*}
  where
  \begin{equation*}
    G = \frac{1}{N} \sum\limits_{n = 1}^N \Psi(x(n)) \Psi(x(n))^T,
    \quad A = \frac{1}{N} \sum\limits_{n = 1}^N \Psi(y(n)) \Psi(x(n))^T.
  \end{equation*}
\end{proposition}

\begin{proof}
  Denote $X = [\Psi(x(1)),\cdots,\Psi(x(N))], Y =
  [\Psi(y(1)),\cdots,\Psi(y(N))]$.
  Then, (\ref{eq:EDMD-problem}) is equivalent to
  \begin{equation*}
    K = \text{argmin}_{\tilde{K} \in \mathbb{R}^{M \times M}} J(\tilde{K})
    = \|Y - \tilde{K}^TX\|_F^2,
  \end{equation*}
  where $\|\cdot\|_F$ is Frobenius norm.
  To minimize $J(\tilde{K})$, we take its derivative with respect to $\tilde{K}$,
  \begin{equation*}
    \frac{\partial J(\tilde{K})}{\partial \tilde{K}^T}
    = - 2(Y - \tilde{K}^TX)X^T,
  \end{equation*}
  and let the derivative to be zero, i.e.,
  $2(Y-\tilde{K}^TX)X^T = 0$,
  which yields
  \begin{equation*}
    YX^T = \tilde{K}^TXX^T.
  \end{equation*}
  By taking the pseudo inverse of $XX^T$,
  we obtain
  \begin{equation*}
    K^T = (YX^T)(XX^T)^+.
  \end{equation*}

  To show $(YX^T)(XX^T)^+ = AG^+$,
  we note:
  \begin{equation*}
    G(i,j) = \frac{1}{N} \sum\limits_{n = 1}^N \psi_i(x(n))\psi_j(x(n)), \quad
    (XX^T)(i,j) = \sum\limits_{n = 1}^N X(i,n)X(j,n) = \sum\limits_{n = 1}^N \psi_i(x(n))\psi_j(x(n)),
  \end{equation*}
  i.e., $XX^T = N \cdot G$.
  Similaryly, $YX^T = N \cdot A$.
  By the properties of the pseudo inverse,
  we have
  \begin{equation*}
    K^T = AG^+ = (YX^T)(XX^T)^+. \qedhere
  \end{equation*}
\end{proof}

\subsubsection{Prediction}


\begin{proposition}
  Suppose $w_j$ is a right eigenvector of matrix $K$ with eigenvalue $\mu_j$.
  Then the function
  \begin{equation}
    \label{eq:expression-of-eigenfunction}
    \varphi_j = w_j^T \Psi
  \end{equation}
  is an approximation of an eigenfunction of $\mathcal{K}$
  with the same eigenvalue $\mu_j$.
\end{proposition}

\begin{proof}
  Direct calculation yields
  \begin{align*}
    \mathcal{K} w_j^T \Psi &= w_j^T \mathcal{K} \Psi\\
                             &= w_j^T K^T \Psi\\
                             &= \mu_j w_j^T \Psi.
  \end{align*}
  where the second step follows from the relation between $\mathcal{K}$ and K,
  and the third step from the definition of the left eigenvector.
  Letting $\varphi_j = w_j^T \Psi$ completes the proof.
\end{proof}


\begin{lemma}
  \label{lem:relation-between-right-and-left-eigenvector}
  Under appropriate scaling,
  the left eigenvector $\xi_j$ and the corresponding
  right eigenvector $w_j$ of a matrix $K$ satisfy
  \begin{equation}
    \label{eq:relation-between-right-and-left-eigenvector}
    \xi_j^{\ast} w_j = 1, \quad \xi_j^{\ast}w_i = 0 \quad  \mathrm{for} ~ i \neq j.
  \end{equation}
\end{lemma}

\begin{proof}
  By the definition of the left eigenvector, we have $\xi_j^\ast K = \lambda_j \xi_j^\ast$.
  Consider the action of $\xi_j^\ast K$ on the right eigenvector $w_i$:
  \begin{equation*}
  \xi_j^\ast K w_i = \xi_j^\ast (\lambda_i w_i) = \lambda_i (\xi_j^\ast w_i).
  \end{equation*}

  On the other hand, using the eigenvalue equation for $\xi_j^\ast$, we also have:
  \begin{equation*}
  \xi_j^\ast K w_i = \lambda_j (\xi_j^\ast w_i).
  \end{equation*}
  By comparing these two expressions, we obtain:
  \begin{equation*}
  \lambda_i (\xi_j^\ast w_i) = \lambda_j (\xi_j^\ast w_i).
  \end{equation*}
  If $\lambda_i \neq \lambda_j$, this equation implies that $\xi_j^\ast w_i = 0$.
  If $\lambda_i = \lambda_j$, then $\xi_j^\ast w_i$ can be any number.
  To satisfy the normalization condition, we typically choose $\xi_j^\ast w_j = 1$.
  Thus, under the appropriate scaling, we have:
  \begin{equation*}
  \xi_j^\ast w_j = 1, \quad \xi_j^\ast w_i = 0 \quad \mathrm{for} ~  i \neq j.
  \end{equation*}
  This completes the proof.
\end{proof}

\begin{proposition}
  Given a matrix $K \in \mathbb{R}^{M \times M}$,
  denote $\Xi = [\xi_1,\cdots,\xi_M]$ and $W = [w_1,\cdots,w_M]$,
  where $\xi_i, w_i$ are the left eigenvectors and right eigenvectors
  of $K$ with eigenvalue $\mu_i$, respectively.
  Then we have
  \begin{equation}
    \label{eq:relation-between-right-and-left-eigenvector-matrix}
    \Xi^{\ast} = W^{-1}.
  \end{equation}
\end{proposition}

\begin{proof}
  By Lemma \ref{lem:relation-between-right-and-left-eigenvector}
  we have
  \begin{equation*}
  (\Xi^{\ast}W)_{i,j} = \xi_i^{\ast} w_j =
  \begin{cases}
    1 & \text{if} ~ i = j;\\
    0 & \text{if} ~ i \neq j,
  \end{cases}
  \end{equation*}
  i.e., $\Xi^{\ast}W = I$, which completes the proof.
\end{proof}

\begin{proposition}
  Consider the full-time observable $\mathbf{g}(x) = x$.
  Assume that for all $g_i(x) \in L^2(\mathcal{M}, \rho)$,
  there exists $V \in \mathbb{C}^{d \times M}$ such that
  \begin{equation*}
    \mathbf{g}(x) = V \Phi(x),
  \end{equation*}
  where $\Phi(x) = [\varphi_1(x),\cdots,\varphi_M(x)]^T$.
\end{proposition}

\begin{proof}
  $\mathbf{g}(x)$ can be written as
  \begin{equation*}
    \mathbf{g}(x) = \left[
      \begin{array}{c}
        g_1(x)\\
        g_2(x)\\
        \vdots\\
        g_d(x)
      \end{array}
    \right] = \left[
      \begin{array}{c}
        e^T_1x\\
        e^T_2x\\
        \vdots\\
        e^T_dx
      \end{array}
    \right],
  \end{equation*}
  where $e_i$ is the $i$th unit vector in $\mathbb{R}^d$.
  Assuming all $g_i(x) \in L^2(\mathcal{M}, \rho)$,
  we have $g_i(x) = \sum\limits_{k = 1}^M \psi_k(x) b_{k,i} = \mathbf{b}_i^T \Psi(x)$,
  which yields
  \begin{equation}
    \label{eq:expression-of-full-time-observable}
    \mathbf{g}(x) = \left[
      \begin{array}{cccc}
        \mathbf{b}_1^T\Psi(x)&\mathbf{b}_2^T\Psi(x)&\cdots&\mathbf{b}_d^T\Psi(x)
      \end{array}
    \right]^T
    = B\Psi(x),
  \end{equation}
  where $B = [\mathbf{b}_1,\cdots,\mathbf{b}_d]^T$.
  Next, we express $\psi_i$ in terms of $\varphi_i$.
  Define
  \begin{equation*}
    \Phi(x) = \left[
      \begin{array}{cccc}
        \varphi_1(x)&\varphi_2(x)&\cdots&\varphi_M(x)
      \end{array}
    \right]^T.
  \end{equation*}
  By (\ref{eq:expression-of-eigenfunction})
  and (\ref{eq:expression-of-full-time-observable}),
  we have
  \begin{equation*}
    \Phi(x) = \left[
      \begin{array}{c}
        w_1^T\\
        \vdots\\
        w_M^T
      \end{array}
    \right] \Psi(x) = W^T \Psi(x).
  \end{equation*}
  Then by (\ref{eq:relation-between-right-and-left-eigenvector-matrix}),
  we have
  \begin{equation}
    \label{eq:right-eigenvector}
    (W^T)^{-1} = \overline{\Xi}
    = \left[
      \begin{array}{cccc}
        \overline{\xi_1}&\overline{\xi_2}&\cdots&\overline{\xi_M}
      \end{array}
    \right],
  \end{equation}
  where $\xi_i$ is the $i$th left eigenvector of $K$ with $\mu_i$.
  Combining (\ref{eq:expression-of-full-time-observable})
  and (\ref{eq:right-eigenvector}),
  we get
  \begin{equation}
    \label{eq:expression-of-g}
    \mathbf{g}(x) = V \Phi(x) = \sum\limits_{k = 1}^M v_k \varphi_k(x),
  \end{equation}
  where $V = [v_1,\cdots,v_M] = B\overline{\Xi} $.
\end{proof}

In actual implementation,
we add fixed functions
$[\mathbf{1}, \mathbf{x}]$ into the dictionary,
which implies
\begin{equation*}
g_i(x) = \mathbf{b}_i^T \Psi(x) = e_{i+1}^T \Psi(x),
\end{equation*}
where $e_i \in \mathbb{R}^M$ has the $i$th element being $1$
and $0$ otherwise.
Thus, the matrix $B$ can be computed as
\begin{equation*}
  B = [e_2, e_3, \cdots, e_{d+1}]^T.
\end{equation*}
With $B$ and $W$ computed, $\mathbf{g}(x)$ can be derived in the form
\begin{equation*}
  \mathbf{g}(x) = \sum\limits_{k = 1}^M v_k\varphi_k(x).
\end{equation*}
Combining this with (\ref{eq:koopman-mode-decomposition-one-step}), we obtain
\begin{equation*}
  x(n) = \sum\limits_{k = 1}^M v_k \mu_k \varphi_k(x(n-1)),
\end{equation*}
which means we can predict the entire trajectory step by step.

\subsection{The EDMD-DL Algorithm}

\subsection{RBF Functions}

\begin{definition}
  Let $(V, \|\cdot\|)$ be a normed linear space.
  A function $\varphi: V \rightarrow \mathbb{R}$ is called
  a \emph{radial basis function (RBF)} if
  there exists a univariate function $\hat{\varphi}: [0, +\infty) \rightarrow \mathbb{R}$
  such that for all $\mathbf{x} \in V$
  \begin{equation*}
   \varphi(\mathbf{x}) = \hat{\varphi}(\|\mathbf{x} - \mathbf{c}\|),
  \end{equation*}
  for some center point $\mathbf{c} \in V$.
\end{definition}

To construct a RBF Dictionary
with $M$ RBF centers and a regularizer $\lambda \in \mathbb{R}$,
we start with a dataset $X \in \mathbb{R}^{N \times d}$,
where $N$ is the number of samples and
$d$ is the number of features.
We first apply k-means clustering to find $M$ centers:
\begin{equation*}
  \{c_1, c_2, \cdots, c_M\} \subset \mathbb{R}^d.
\end{equation*}
For each data point $\mathbf{x}_i \in X$,
the distance between $\mathbf{x}_i$ and $c_j$ is
given by $r_{ij} = \|\mathbf{x}_i - c_j\|$.
The RBF value is then computed as
\begin{equation*}
  \phi_{ij} = (r_{ij})^2 \log(r_{ij} + \lambda),
\end{equation*}
where $\phi_{ij}$ is the RBF value for the
$i$th data point and the $j$-th center.
Collecting all the RBF values yields a matrix $\Phi \in \mathbb{R}^{N \times M}$
\begin{equation*}
  \Phi = \left[
    \begin{array}{cccc}
      \phi_{11}&\phi_{12}&\cdots&\phi_{1M}\\
      \phi_{21}&\phi_{22}&\cdots&\phi_{2M}\\
      \vdots & \vdots & \ddots & \vdots\\
      \phi_{N1}&\phi_{N2}&\cdots&\phi_{NM}
    \end{array}
  \right].
\end{equation*}
To maintain consistency with the trainable dictionary,
we add a column of ones and
include the original data matrix $X$:
\begin{equation*}
  \text{output} = \left[
    \begin{array}{ccc}
      \mathbf{1}&X&\Phi
    \end{array}
  \right],
\end{equation*}
where $\mathbf{1} \in \mathbb{R}^{N\times 1}$ is a vector with all elements
equal to one.

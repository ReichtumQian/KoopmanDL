
\documentclass[en, bibend=bibtex]{elegantpaper}

\title{KoopmanDL Documentation}
\author{Yixiao Qian}
\institute{Zhejiang University}
\date{\today}
\setcounter{tocdepth}{3}
% \everymath{\displaystyle}
\usepackage{esint}
\theoremstyle{plain}
\usepackage{miniplot}
\usepackage{algorithm}
\usepackage{algorithmic}

\renewcommand{\proofname}{\textsl {Proof}}

\usepackage{bm}
\makeatletter
\def\renewtheorem#1{%
  \expandafter\let\csname#1\endcsname\relax
  \expandafter\let\csname c@#1\endcsname\relax
  \gdef\renewtheorem@envname{#1}
  \renewtheorem@secpar
}
\def\renewtheorem@secpar{\@ifnextchar[{\renewtheorem@numberedlike}{\renewtheorem@nonumberedlike}}
\def\renewtheorem@numberedlike[#1]#2{\newtheorem{\renewtheorem@envname}[#1]{#2}}
\def\renewtheorem@nonumberedlike#1{
  \def\renewtheorem@caption{#1}
  \edef\renewtheorem@nowithin{\noexpand\newtheorem{\renewtheorem@envname}{\renewtheorem@caption}}
  \renewtheorem@thirdpar
}
\def\renewtheorem@thirdpar{\@ifnextchar[{\renewtheorem@within}{\renewtheorem@nowithin}}
\def\renewtheorem@within[#1]{\renewtheorem@nowithin[#1]}
\makeatother

\begin{document}
\maketitle
\tableofcontents
\newpage

\section{Preliminary}

\subsection{Koopman Operator}

Consider a measure space $(\mathcal{M}, \mathcal{F}, \rho)$,
on which we define a dynamical system
\begin{equation*}
  x(n+1) = f(x(n)), \quad x(n) \in \mathcal{M}, \quad n \geq 0.
\end{equation*}

\begin{definition}
  Consider the Hilbert space
  \begin{equation*}
  L^2(\mathcal{M}, \rho) = \left\{ \psi: \mathcal{M} \rightarrow \mathbb{C}:
    \|\psi\|_{L^2(\mathcal{M},\rho)} < \infty \right\},
  \end{equation*}
  for all $\psi \in L^2(\mathcal{M}, \rho)$, it is called an \emph{observable}.
\end{definition}

\begin{definition}
  For any observable $\psi \in L^2(\mathcal{M}, \rho)$,
  the \emph{Koopman operator} maps it to another observable
  \begin{equation*}
    \mathcal{K} \psi = \psi \circ f.
  \end{equation*}
\end{definition}

\subsection{The EDMD Algorithm}

The EDMD algorithm aims to find a finite-dimensional
representation of the Koopman operator $\mathcal{K}$.
Picking a dictionary $D = \{\psi_1,\psi_2,\cdots,\psi_M\}$,
where
\begin{equation*}
  \psi_i: \mathcal{M} \rightarrow \mathbb{R},
  \quad \text{for} \quad i = 1,2,\cdots, M,
\end{equation*}
we consider the span $U(D) = \text{span}\{\psi_1,\cdots,\psi_M\} = \{a^T\Psi:a \in \mathbb{C}^M\}$,
where $\Psi = [\psi_1,\psi_2,\cdots,\psi_M]^T$.
Assume that for all $\psi \in U(D)$,
$\mathcal{K} \psi \in U(D)$,
then $\mathcal{K}$ is a linear transformation in the space $U(D)$,
which means $\mathcal{K}$ can be represented
by a matrix $K \in \mathbb{R}^{M \times M}$:
\begin{equation*}
  K \left[
    \begin{array}{c}
      \psi_1\\
      \psi_2\\
      \vdots\\
      \psi_M
    \end{array}
  \right] = \left[
    \begin{array}{c}
      \psi_1 \circ f\\
      \psi_2 \circ f\\
      \vdots\\
      \psi_M \circ f
    \end{array}
  \right].
\end{equation*}
However, since the nonlinear properties of $f$,
the Koopman operator $\mathcal{K}$ is not always equivalent to the matrix $K$.
But in that case, we can still consider $K$ as a finite-dimensional
approximation of $\mathcal{K}$.


From the view of data science,
given a set of data points $\{(x(n), y(n))\}_{n = 1}^N$
with $y(n) = f(x(n))$,
we can find $K$ by solving an optimization problem
\begin{equation}
  \label{eq:EDMD-problem}
  K = \text{argmin}_{\tilde{K} \in \mathbb{R}^{M \times M}} J(\tilde{K})
  = \sum\limits_{n = 1}^N \|\Psi(y(n)) - \tilde{K} \Psi(x(n))\|_2^2,
\end{equation}
where $\Psi(x) = [\psi_1(x), \psi_2(x),\cdots,\psi_M(x)]^T$.

\begin{proposition}
  The solution of (\ref{eq:EDMD-problem}) is
  \begin{equation*}
    K = AG^+,
  \end{equation*}
  where
  \begin{equation*}
    G = \frac{1}{N} \sum\limits_{n = 1}^N \Psi(x(n)) \Psi(x(n))^T,
    \quad A = \frac{1}{N} \sum\limits_{n = 1}^N \Psi(x(n)) \Psi(y(n))^T.
  \end{equation*}
\end{proposition}

\begin{proof}
  Denote $X = [\Psi(x(1)),\cdots,\Psi(x(N))], Y =
  [\Psi(y(1)),\cdots,\Psi(y(N))]$,
  then (\ref{eq:EDMD-problem}) is equivalent to
  \begin{equation*}
    K = \text{argmin}_{\tilde{K} \in \mathbb{R}^{M \times M}} J(\tilde{K})
    = \|Y - \tilde{K}X\|_F^2,
  \end{equation*}
  where $\|\cdot\|_F$ is Frobenius norm.
  To minimize $J(\tilde{K})$, we take its derivative with respect to $\tilde{K}$,
  \begin{equation*}
    \frac{\partial J(\tilde{K})}{\partial \tilde{K}}
    = - 2(Y - \tilde{K}X)X^T,
  \end{equation*}
  and let the derivative to be zero, i.e.,
  $2(Y-\tilde{K}X)X^T = 0$,
  which yields
  \begin{equation*}
    YX^T = \tilde{K}XX^T.
  \end{equation*}
  By taking the pseudo inverse of $XX^T$,
  we obtain
  \begin{equation*}
    K = (YX^T)(XX^T)^+.
  \end{equation*}

  Now we show that $(YX^T)(XX^T)^+ = AG^+$,
  by the definition of matrix multiplication
  \begin{equation*}
    G(i,j) = \frac{1}{N} \sum\limits_{n = 1}^N \psi_i(x(n))\psi_j(x(n)), \quad
    (XX^T)(i,j) = \sum\limits_{n = 1}^N X(i,n)X(j,n) = \sum\limits_{n = 1}^N \psi_i(x(n))\psi_j(x(n)),
  \end{equation*}
  i.e., $XX^T = N \cdot G$.
  The same reasoning yields $YX^T = N \cdot A$.
  By the properties of pseudo inverse,
  we have
  \begin{equation*}
    K = AG^+ = (YX^T)(XX^T)^+. \qedhere
  \end{equation*}
\end{proof}


\subsection{The EDMD-DL Algorithm}

\subsection{RBF Functions}

\begin{definition}
  Let $(V, \|\cdot\|)$ be a normed linear space,
  a function $\varphi: V \rightarrow \mathbb{R}$ is called
  a \emph{radial basis function (RBF)} if
  there exists a univariate function $\hat{\varphi}: [0, +\infty) \rightarrow \mathbb{R}$
  such that for all $\mathbf{x} \in V$
  \begin{equation*}
   \varphi(\mathbf{x}) = \hat{\varphi}(\|\mathbf{x} - \mathbf{c}\|),
  \end{equation*}
  for some center point $\mathbf{c} \in V$.
\end{definition}

Assume we want to build a RBF Dictionary
with $M$ RBF centers and a regularizer $\lambda \in \mathbb{R}$.
Given a dataset $X \in \mathbb{R}^{N \times D}$,
where $N$ is the number of samples and
$D$ is the number of features,
we first apply k-means clustering to find $M$ centers:
\begin{equation*}
  \{\mu_1, \mu_2, \cdots, \mu_M\} \subset \mathbb{R}^D.
\end{equation*}
For each data point $\mathbf{x}_i \in X$,
the distance between $\mathbf{x}_i$ and $\mu_j$ is
$r_{ij} = \|\mathbf{x}_i - \mu_j\|$,
then the RBF value is computed as
\begin{equation*}
  \phi_{ij} = (r_{ij})^2 \log(r_{ij} + \lambda),
\end{equation*}
where $\phi_{ij}$ is the RBF value for the
$i$-th data point and the $j$-th center.
Collecting all the RBF values yields a matrix $\Phi \in \mathbb{R}^{N \times M}$
\begin{equation*}
  \Phi = \left[
    \begin{array}{cccc}
      \phi_{11}&\phi_{12}&\cdots&\phi_{1M}\\
      \phi_{21}&\phi_{22}&\cdots&\phi_{2M}\\
      \vdots & \vdots & \ddots & \vdots\\
      \phi_{N1}&\phi_{N2}&\cdots&\phi_{NM}
    \end{array}
  \right].
\end{equation*}
To keep the same as Trainable Dictionary,
we add one column of all-one vector and
the projection part
\begin{equation*}
  \text{output} = \left[
    \begin{array}{ccc}
      \mathbf{1}&X&\Phi
    \end{array}
  \right],
\end{equation*}
where $\mathbf{1} \in \mathbb{R}^{N\times 1}$ is a vector with all elements
equal to one.


\section{Dictionary}

\subsection{Class Dictionary}

\begin{itemize}
\item Brief description: A dictionary is vector function
  $\Psi: \mathbb{R}^d \rightarrow \mathbb{R}^M$.
  Batch operation is also supported $\Psi: \mathbb{R}^{N \times d} \rightarrow
  \mathbb{R}^{N \times M}$
\item Attributes:
  \begin{itemize}
  \item \lstinline|_func|: The vector function.
  \item \lstinline|_M|: The output dimension of the vector function.
  \end{itemize}
\item Methods:
  \begin{itemize}
  \item \lstinline|__init__(self, M, func)|: initialize the dictionary.
    Note \lstinline|func| must support batch operation
  \item \lstinline|__call__(self, x)|: apply the dictionary,
    the input must satisfy $x \in \mathbb{R}^d$ or $x \in \mathbb{R}^{N \times d}$.
  \end{itemize}
\end{itemize}

\subsection{Class TrainableDictionary(Dictionary)}

\begin{itemize}
\item Brief description: A dictionary that contains a trainable neural network.
  It is a vector function $\mathbb{R}^d \rightarrow \mathbb{R}^M$,
  the output contains non-trainable outputs $\mathbf{1}$ and $x$.
\item Attributes:
  \begin{itemize}
  \item \lstinline|__optimizer|: the optimizer of the trainable neural network.
  \end{itemize}
\item Methods:
  \begin{itemize}
  \item \lstinline|__init__(self, M, func, optimizer)|:
  Note \lstinline|func| must be an instance of \lstinline|torch.nn.Module|.
  \item \lstinline|train(self, data_loader, loss_func, n_epochs)|:
  train the neural network.
  \end{itemize}
\end{itemize}

\subsection{Class RBFDictionary(Dictionary)}

\begin{itemize}
\item Brief description: 
\end{itemize}

\section{Solver}

\subsection{Class EDMDSolver}

\begin{itemize}
\item Brief description: Implementation of EDMD algorithm,
  also acts as the base class of \lstinline|EDMDDLSolver|.
\item Attributes:
  \begin{itemize}
  \item \lstinline|__init__(self, dictionary)|
  \item \lstinline|compute_K(self, data_x, data_y)|:
    Apply the formula $K = AG^+$ to compute $K$.
  \end{itemize}
\item Methods:
\end{itemize}



\subsection{Class EDMDDLSolver(EDMDSolver)}

\begin{itemize}
\item Brief description: Implementation of EDMD-DL algorithm.
\item Attributes:
  \begin{itemize}
  \item \lstinline|__regularizer|: the regularizer $\lambda$.
  \end{itemize}
\item Methods:
  \begin{itemize}
  \item \lstinline|__init__(self, dictionary, regularizer)|
  \item \lstinline|compute_K(self, data_x, data_y)|:
    Apply the formula $K = A(G + \lambda I)^+$ to compute $K$.
  \item \lstinline|solve(self, data_x, data_y, n_epochs, batch_size, tolerance)|:
    Apply EDMD-DL algorithm to solve the system.
  \end{itemize}
\end{itemize}

\section{Net and Dataset}

\subsection{Class TanhNet}

\begin{itemize}
\item Brief description: A simple full-connected network with tanh activation,
  achieved by pytorch.
\item Attributes:
  \begin{itemize}
  \item \lstinline|__network|: the network.
  \end{itemize}
\item Methods:
  \begin{itemize}
  \item \lstinline|__init__(self, input_dim, output_dim, hidden_layer_sizes)|
  \item \lstinline|forward(self, x)|
  \end{itemize}
\end{itemize}

\subsection{Class TanhNetWithNonTrainable(TanhNet)}

\begin{itemize}
\item Brief description: Full-connected network designed for the 
  \lstinline|TrainableDictionary|,
  which contains a non-trainable outputs,
  and a non-trainable layer.
\item Attributes:
  \begin{itemize}
  \item \lstinline|__output_layer|: the non-trainable layer.
  \end{itemize}
\item Methods:
  \begin{itemize}
  \item \lstinline|__init__(self, input_dim, output_dim, hidden_layer_sizes, n_nontrainable)|
  \item \lstinline|forward(self, x)|
  \item \lstinline|set_output_layer(self, weight)|
  \end{itemize}
\end{itemize}

\section{ODE and Flowmap}





\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:

